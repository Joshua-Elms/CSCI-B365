{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means++ \n",
    "I spoke with Hasan at length about this assignment, and he told me that I would be able to receive extra credit for implementing K-Means++. My code is also heavily optimized for performance, about as much as I could do without switching languages.\n",
    "\n",
    "I will cover my changes at each step they appear throughout my code, but for a brief summary of how I structured my program, see below.\n",
    "\n",
    "## Algorithm Documentation <br>\n",
    "\n",
    "### Dependencies\n",
    "* numpy as np\n",
    "    * I use numpy exclusively for operations; I created one system that employed pandas, but it was very slow and inefficient, as well as hard to understand\n",
    "<br><br>\n",
    "* NoneType from types\n",
    "    * Used to do input validation\n",
    "<br><br>\n",
    "* math\n",
    "    * used for sqrt \n",
    "<br><br>\n",
    "* pandas\n",
    "    * used only for reading in data, then converting to numpy arrays\n",
    "<br><br>\n",
    "\n",
    "### Functions\n",
    "* gen_uniform_data (not in given framework)\n",
    "    * Samples from a uniform distribution to generate synthetic data with parameters:\n",
    "        * len: Number of data points\n",
    "        * dims: Number of dimensions\n",
    "        * low: Lower boundary for uniform distr.\n",
    "        * high: Upper boundary for uniform distr.\n",
    "        * rng: np.random.default_rng object, allows random seed to be set for repeatable and random results\n",
    "<br><br>\n",
    "* euclidean_distance_matrix (loosely corresponds to given \"euclidean_distance\")\n",
    "    * Calculate squared euclidean distance (L2 Norm) for two given input matrices\n",
    "    * I calculate squared euclidean distance because:\n",
    "        * It is less computationally intensive that standard euclidean distance\n",
    "        * It preserves relationships, if not values; i.e. x, y = 1, 2 and 1 < 2, so x^2 < y^2\n",
    "        * In most of the distance related operations for k-means, we are simply determining which point has minimum distance\n",
    "        * If ever I need to use true euclidean distance, I can simply use np.sqrt to efficiently take the sqrt of every element\n",
    "    * I used numpy for vectorized operations so that I could reduce runtime caused by pandas iterrows() inefficiency\n",
    "<br><br>\n",
    "* manhattan_distance_matrix (loosely corresponds to given \"manhattan_distance\")\n",
    "    * Calculate squared manhattan distance (L1 Norm) for two given input matrices\n",
    "    * Everything true of the above function applies here \n",
    "<br><br>\n",
    "* initialization (loosely corresponds to given \"random_centers\", but for k-means++)\n",
    "    * See link: https://en.wikipedia.org/wiki/K-means%2B%2B\n",
    "    * At its most basic:\n",
    "        * First centroid given value of randomly selected point from data\n",
    "        * Subsequent centroids are selected from a distribution that favors points furthest from any other centroid\n",
    "    * Algorithm optimized to avoid unnecessary extra calculation, with my benchmarks showing a runtime improvement of around 15% from naive \n",
    "<br><br>\n",
    "* assignment (corresponds to given \"assign_centers\")\n",
    "    * Takes as input the data, centroids, and distance measure (euclidean or manhattan)\n",
    "    * Returns an nd.array associating the index of each point with a cluster label \n",
    "<br><br>\n",
    "* update (corresponds to given \"recalculate_centers\" and \"cost)\n",
    "    * Takes as input the data, centroids, and labels from assignment()\n",
    "    * Two outputs:\n",
    "        * Array of new centroid locations according to distance measure and cluster labels\n",
    "        * SSE (average euclidean distance traveled by all centroids from iteration i to i + 1)\n",
    "    * I included \"cost\" in this function because it eliminates the need for a separate function to be called and more data to be passed\n",
    "    * Also, SSE is best calculated after each set of centroids is updated anyway, so this occurs at the perfect time \n",
    "<br><br>\n",
    "* kmeans (corresponds to given \"kmeans\")\n",
    "    * Includes all parameters from the given function, as well as others to control every aspect of the algorithm and data generation\n",
    "        * k: Number of clusters\n",
    "        * threshold: Average amount of centroid movement between iterations considered acceptable\n",
    "        * max_iterations: Maximum number of iterations before clusters are returned\n",
    "        * plus_plus: Whether to use my k-means++ implementation or generic k-means\n",
    "        * dist_function: Either \"euclidean\" or \"manhattan\"\n",
    "        * dims: Integer, dimensionality of data passed (if incorrect when passing outside data, algorithm will fail)\n",
    "        * data: Synthetic according to parameters below if None, else \"dims\" dimensional numerical data\n",
    "            * size: Number of synthetic data points\n",
    "            * lower: Lower limit of uniform distribution\n",
    "            * upper: Upper limit of uniform distribution\n",
    "            * dims: see above mention\n",
    "            * seed: np.random.default_rng() seed value, allows for repeated randomness  \n",
    "    * Returns lists containing all predicted centroid values and vector of integers 0:k-1 representing cluster labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "from types import NoneType, FunctionType\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic uniform data\n",
    "def gen_uniform_data(len, dims, low, high, rng):\n",
    "    data = rng.integers(low, high, size=(len,dims),endpoint=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non ++ implementation cluster generator\n",
    "def _gen_clusts(k, dims, low, high, rng):\n",
    "    return rng.integers(low, high, size=(k,dims),endpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized squared euclidean distance\n",
    "def euclidean_distance_matrix(points1, points2, dims):\n",
    "    # calculate the difference between points at each dimensions, add axis into one set of points to allow operation\n",
    "    diff = points2 - points1[:, np.newaxis, :]\n",
    "\n",
    "    # Calculate L2 norm for each group of points\n",
    "    norms = sum([diff[..., i] ** 2 for i in range(dims)])\n",
    "    return norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized squared manhattan distance\n",
    "def manhattan_distance_matrix(points1, points2, dims):\n",
    "    # calculate the difference between points at each dimensions, add axis into one set of points to allow operation\n",
    "    diff = points2 - points1[:, np.newaxis, :]\n",
    "\n",
    "    # Calculate L1 norm for each group of points\n",
    "    norms = np.square(sum([abs(diff[..., i]) for i in range(dims)]))\n",
    "    return norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# very long initialization function, makes this k-means++\n",
    "def initialization(data, k, rng, dist):\n",
    "    unselected = [i for i in range(len(data))] # indices of all non-selected elements (all when run)\n",
    "    selected = []\n",
    "    dims = data.shape[-1]\n",
    "\n",
    "    # find first point, random\n",
    "    rows = data.shape[0]\n",
    "    first_centroid = rng.integers(0, rows, endpoint=False)\n",
    "\n",
    "    # remove that point from list of unselected ones, add to selected\n",
    "    selected.append(unselected.pop(first_centroid))\n",
    "\n",
    "    # first iteration of k-means++ selection process, for loop proceeding this covers the rest\n",
    "    distance_matrix = dist(data[unselected, :], data[selected, :], dims)\n",
    "    flat_distances = distance_matrix.flatten()\n",
    "    sum = np.sum(flat_distances)\n",
    "    probabilities = flat_distances / sum\n",
    "    new_centroid = np.random.choice(unselected, p=probabilities) \n",
    "    selected.append(unselected.pop(unselected.index(new_centroid)))\n",
    "\n",
    "    # only to k-2 because the first time we do this process, there is no need to determine closest centroid; there's only one\n",
    "    for centroid_index in range(k-2):\n",
    "\n",
    "        # generate distance matrix to be used for rest of iteration\n",
    "        new_column_of_distance_matrix = dist(data[unselected, :], data[selected[-1], :], dims)\n",
    "\n",
    "        # remove row from previous distance matrix\n",
    "        index_offset = 0\n",
    "        for centroid_index in selected[:-1]:\n",
    "            if centroid_index < new_centroid:\n",
    "                index_offset += 1\n",
    "        \n",
    "        distance_matrix = np.delete(distance_matrix, new_centroid - index_offset, axis=0)\n",
    "\n",
    "        # concat matrices together\n",
    "        distance_matrix = np.concatenate((distance_matrix, new_column_of_distance_matrix), axis=1)\n",
    "\n",
    "        # get the smallest distances from each point to a centroid\n",
    "        dist_points_to_closest_centroids = np.amin(distance_matrix, axis=1)\n",
    "\n",
    "        # final step for initialization in k-means++; actually choose new centroid with probability proportional to distance^2\n",
    "        # our distance has been squared from the beginning for easier computation, so this removes a lot of compute time\n",
    "        flat_distances = dist_points_to_closest_centroids.flatten()\n",
    "        sum = np.sum(flat_distances)\n",
    "        probabilities = flat_distances / sum\n",
    "        new_centroid = np.random.choice(unselected, p=probabilities) \n",
    "\n",
    "        # add new centroid index to centroid list\n",
    "        selected.append(unselected.pop(unselected.index(new_centroid)))\n",
    "\n",
    "    # finally, we have a list of indices of all points that have been chosen as centroids through k-means++\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster assignment step \n",
    "def assignment(data, centroids, k, dims, distance):\n",
    "    # get new array of all cluster assignments\n",
    "    distance_matrix = distance(data, centroids, dims)\n",
    "    cluster_labels = np.argmin(distance_matrix, axis=1)[:, np.newaxis]\n",
    "    indices_and_cluster_labels = np.concatenate((np.arange(data.shape[0])[:, np.newaxis], cluster_labels), axis=1)\n",
    "\n",
    "    return indices_and_cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroid update step as well as SSE/cost function\n",
    "def update(data, indices_and_cluster_labels, current_centroids, k, distance):\n",
    "    # create list of k arrays, each of which is just just all the points in the i-th cluster\n",
    "    grouped_by_cluster = []\n",
    "    for i in range(k):\n",
    "        condition = np.extract(indices_and_cluster_labels[:, 1]==i, indices_and_cluster_labels[:, 0])\n",
    "        grouped_by_cluster.append(data[condition, :])\n",
    "\n",
    "    new_centroids_list = [np.mean(arr, axis=0) for arr in grouped_by_cluster]\n",
    "    new_centroids_arr = np.stack(new_centroids_list, axis=0)\n",
    "\n",
    "    if isinstance(distance, FunctionType):\n",
    "        if (distance.__name__)[:3] == \"euc\":\n",
    "            SSE = (1 / k) * math.sqrt(np.sum((new_centroids_arr - current_centroids) ** 2))\n",
    "\n",
    "        elif (distance.__name__)[:3] == \"man\":\n",
    "            SSE = (1 / k) * np.sum(np.abs(new_centroids_arr - current_centroids))\n",
    "    \n",
    "    else: \n",
    "        if (distance)[:3] == \"euc\":\n",
    "            SSE = (1 / k) * math.sqrt(np.sum((new_centroids_arr - current_centroids) ** 2))\n",
    "\n",
    "        elif (distance)[:3] == \"man\":\n",
    "            SSE = (1 / k) * np.sum(np.abs(new_centroids_arr - current_centroids))\n",
    "\n",
    "    return new_centroids_arr, SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# controls k-means execution\n",
    "def kmeans(k, threshold=0.001, dist_function=\"euclidean\",\n",
    "            data=None, size=1000, lower=0, upper=100, \n",
    "            dims=2, max_iterations=25, seed=None, plus_plus=True):\n",
    "\n",
    "    if dist_function.strip().lower()[:3] == \"euc\":\n",
    "        dist = euclidean_distance_matrix\n",
    "\n",
    "    elif dist_function.strip().lower()[:3] == \"man\":\n",
    "        dist = manhattan_distance_matrix\n",
    "\n",
    "    else: \n",
    "        raise(NameError(\"Enter a valid distance metric name\"))\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    if isinstance(data, NoneType):\n",
    "        data = gen_uniform_data(size, dims, lower, upper, rng)\n",
    "\n",
    "    else: \n",
    "        data = np.asarray(data)\n",
    "\n",
    "    if plus_plus:\n",
    "        centroids = data[initialization(data, k, rng, dist), :]\n",
    "\n",
    "    else: \n",
    "        centroids = _gen_clusts(k, dims, lower, upper, rng)\n",
    "\n",
    "    labels = assignment(data, centroids, k, dims, dist)\n",
    "\n",
    "    step = 0\n",
    "    sse = threshold + 1 # ensures that sse below threshold prevent the while loop from ever executing\n",
    "    while sse > threshold and step < max_iterations:\n",
    "        centroids, sse = update(data, labels, centroids, k, dist)\n",
    "        labels= assignment(data, centroids, k, dims, dist)\n",
    "        step += 1\n",
    "\n",
    "\n",
    "    return centroids, labels[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# given helper function with minor modifications to fit my framework\n",
    "\n",
    "# Given predicted centers, predicted labels, true centers and true labels\n",
    "# finds a mapping between predicted labels and actual labels\n",
    "# and returns the number of true predictions and their percentage\n",
    "\n",
    "def evaluation(data, pred_centers, pred_labels, true_labels, dist_function=\"euclidean\"):\n",
    "    # The predicted centers and actual centers may not match\n",
    "    # The center we label as i can be equal to a different index j in the actual centers and labels\n",
    "    # Therefore we need to do a mapping, so that we can calculate the accuracy.\n",
    "    mapping = {}\n",
    "    k = pred_centers.shape[0]\n",
    "    K = len(np.unique(true_labels))\n",
    "    dist_func_str = dist_function\n",
    "\n",
    "    if dist_function==\"euclidean\":\n",
    "        dist_function = lambda x, y: math.sqrt(sum([(x[i] - y[i])**2 for i in range(len(x))]))\n",
    "    \n",
    "    elif dist_function==\"manhattan\":\n",
    "        dist_function = lambda x, y: (sum([abs(x[i] - y[i]) for i in range(len(x))]))\n",
    "\n",
    "    true_centers, _ = update(data, true_labels, k=K, current_centroids=pred_centers, distance=dist_func_str)\n",
    "    # To achieve a mapping, simply try to find which center actually belongs to which cluster\n",
    "    # by mapping predicted centers to true centers, based on the distance.\n",
    "    for c in range(k):\n",
    "        # Distance off predicted center to true center\n",
    "        min_dist = math.inf\n",
    "        idx = c\n",
    "        for tc in range(K) :\n",
    "            dist = dist_function(pred_centers[c], true_centers[:, tc])\n",
    "            if min_dist > dist:\n",
    "                min_dist = dist\n",
    "                idx = tc\n",
    "        mapping[c] = idx\n",
    "        \n",
    "    accurate_points = 0\n",
    "    for i in range(len(pred_labels)):\n",
    "        # Get the actual cluster label\n",
    "        mapped_value = mapping[pred_labels[i]]\n",
    "        if mapped_value == true_labels[i]:\n",
    "            accurate_points += 1\n",
    "            \n",
    "    accuracy = accurate_points/len(pred_labels)\n",
    "    print(\"Accuracy is \" + str(accuracy*100) + \"%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/95/7xkz0c3d73z_7dvzl3nvx1g80000gn/T/ipykernel_37638/93045501.py:4: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  data = mydata.drop(\"class\", 1)\n"
     ]
    }
   ],
   "source": [
    "# data processing step\n",
    "mydata = pd.read_csv(\"wine.data\")\n",
    "true_labels = mydata[\"class\"].to_numpy().astype(int) - 1\n",
    "data = mydata.drop(\"class\", 1)\n",
    "true_labels = np.concatenate((np.arange(mydata.shape[0])[:, np.newaxis], true_labels[:, np.newaxis]), axis=1)\n",
    "# data normalization\n",
    "data=(data-data.mean())/data.std()\n",
    "data = data.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_centers, pred_labels = kmeans(data=data, dist_function=\"euclidean\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (177,13) (2,13) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [183]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mevaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_centers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdist_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meuclidean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [180]\u001b[0m, in \u001b[0;36mevaluation\u001b[0;34m(data, pred_centers, pred_labels, true_labels, dist_function)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dist_function\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanhattan\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     20\u001b[0m     dist_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x, y: (\u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mabs\u001b[39m(x[i] \u001b[38;5;241m-\u001b[39m y[i]) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(x))]))\n\u001b[0;32m---> 22\u001b[0m true_centers, _ \u001b[38;5;241m=\u001b[39m \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_centroids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_centers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdistance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdist_func_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# To achieve a mapping, simply try to find which center actually belongs to which cluster\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# by mapping predicted centers to true centers, based on the distance.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# Distance off predicted center to true center\u001b[39;00m\n",
      "Input \u001b[0;32mIn [178]\u001b[0m, in \u001b[0;36mupdate\u001b[0;34m(data, indices_and_cluster_labels, current_centroids, k, distance)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (distance)[:\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuc\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 21\u001b[0m         SSE \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m k) \u001b[38;5;241m*\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(np\u001b[38;5;241m.\u001b[39msum((\u001b[43mnew_centroids_arr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcurrent_centroids\u001b[49m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (distance)[:\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     24\u001b[0m         SSE \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m k) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msum(np\u001b[38;5;241m.\u001b[39mabs(new_centroids_arr \u001b[38;5;241m-\u001b[39m current_centroids))\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (177,13) (2,13) "
     ]
    }
   ],
   "source": [
    "evaluation(data, pred_centers, pred_labels, true_labels, dist_function=\"euclidean\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bebecaaf41780d017764b59d9fbf95c0501ae565161cbfe7bdcfa8b1930c3dd6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('rising_sun')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
